---
title: "Logistic Regression"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## Load the necessary packages

```{r packages}
library(readr)
library(tidyverse)
library(tidymodels)
```

## Load the data

### The data

The data we are working with is again from the OpenIntro site.
From OpenIntro's [description of the data](https://www.openintro.org/data/index.php?data=resume):

This experiment data comes from a study that sought to understand the influence of race and gender on job application callback rates.The study monitored job postings in Boston and Chicago for several months during 2001 and 2002 and used this to build up a set of test cases. 

Over this time period, the researchers randomly generating résumés to go out to a job posting, such as years of experience and education details, to create a realistic-looking résumé.They then randomly assigned a name to the résumé that would communicate the applicant's gender and race.

The first names chosen for the study were selected so that the names would predominantly be recognized as belonging to black or white individuals.

For example, Lakisha was a name that their survey indicated would be interpreted as a black woman, while Greg was a name that would generally be interpreted to be associated with a white male.

```{r data}
resume <- read_csv("resume.csv")
```
This study is classified as an experiment because it involves the active intervention of researchers who systematically manipulated key variables to investigate their effects on job application callback rates. Specifically, the researchers randomly generated resumes, varying only the names to imply different races and genders, and submitted these to various job postings. This method of random assignment and controlled manipulation—characteristics unique to experimental designs—aims to establish a causal relationship between the perceived race and gender of applicants and the likelihood of receiving a callback. Unlike observational studies, where researchers passively observe outcomes without intervening, this experiment's deliberate intervention allows for a clearer understanding of the influence of racial and gender biases in hiring practices.


The variable received_callback is a binary variable, which is a specific type of categorical variable. Binary variables can take on only two possible values, which represent two distinct categories or outcomes.

In the context of this study, the two values that received_callback can take would typically be:

1 (or another value such as True): Indicating that the applicant did receive a callback for the job application.

0 (or another value such as False): Indicating that the applicant did not receive a callback for the job application.

The values of this variable represent whether or not an applicant's submission for a job led to a callback from the employer, which is the primary outcome of interest in this experiment. The purpose of using such a variable is to measure the success of the job applications in a quantifiable way, allowing the researchers to assess the impact of race and gender (inferred through the names on the resumes) on employers' callback decisions. This binary outcome serves as a straightforward and effective way to analyze the data and draw conclusions about potential biases in the hiring process.


```{r plots}
resume %>% 
  ggplot(aes(x=received_callback==1)) + 
  geom_bar() + 
  theme_bw() + 
  ylab("Count") + 
  xlab("Received Call Back") + 
  ggtitle("Number of Received Call Back")

```


```{r callback_frequency_table}
resume %>% 
  mutate(received_callback = case_when(
    received_callback == 0 ~ "No",
    received_callback == 1 ~ "Yes"
  )) %>% 
  count(received_callback) %>% 
  mutate(percent = round(n / sum(n) * 100, 2)) %>% 
  knitr::kable()
```



The information from Output 3, which showed a graphical representation with a substantial imbalance between the "FALSE" (no callback) and "TRUE" (received callback) categories, and Output 4, which provided a numerical summary, both illustrate a stark contrast in callback rates. Specifically, only 8.05% of the résumés received a callback, while a vast majority, 91.95%, did not. This suggests a significantly low callback rate for job applications within the study, hinting at potential underlying factors affecting the likelihood of a callback that may warrant further investigation. 


## Probability and odds

The probability that a randomly selected résumé/person will be called back is approximately 8.05%.

The odds of someone getting a call back is 8.75%


## Logistic regression

Logistic regression is one form of a *generalized linear model*.
For this type of model, the outcome/response variable takes one one of two levels (sometimes called a binary variable or a two-level categorical variable).

In our activity, $Y_i$ takes the value 1 if a résumé receives a callback and 0 if it did not.
Generally, we will let the probability of a "success" (a 1) be $p_i$ and the probability of a "failure" (a 0) be $1 - p_i$.
Therefore, the odds of a "success" are:

$$
\frac{Pr(Y_i = 1)}{Pr(Y_i = 0)} = \frac{p_i}{1-p_i}
$$

From your reading, you saw that we use the *logit function* (or *log odds*) to model binary outcome variables:

$$
\begin{equation*}
\log\left(\frac{p_i}{1-p_i}\right) = \beta_0 + \beta_1 X
\end{equation*}
$$

To keep things simpler, we will first explore a logistic regression model with a two-level categorical explanatory variable: `race` - the inferred race associated to the first name on the résumé.

Below is a two-way table (also known as a contingency table or crosstable), where the rows are the response variable levels, the columns are the explanatory variable levels, and the cells are the percent (and number of in parentheses).

Note that the values in each column add to 100%.

```{r crosstable}
resume %>% 
  mutate(received_callback = case_when(
    received_callback == 0 ~ "No",
    received_callback == 1 ~ "Yes"
  ),
  race = case_when(
    race == "black" ~ "Black",
    race == "white" ~ "White"
  )) %>% 
  group_by(race, received_callback) %>% 
  summarise(n = n()) %>% 
  mutate(percent = round(n / sum(n) * 100, 2),
         percent_n = glue::glue("{percent} ({n})")) %>% 
  select(received_callback, race, percent_n) %>% 
  pivot_wider(
    names_from = race,
    values_from = percent_n
  ) %>% 
  knitr::kable()
```

The probability that a randomly selected résumé/person perceived as Black will be called back is given by the percentage of Black applicants who received a 'Yes' for a callback. This is 6.45%.

The probability that a randomly selected résumé/person perceived as Black will be called back is approximately 6.89%.

This process of calculating conditional (e.g., if a résumé/person perceived as Black is called back) odds will be helpful as we fit our logistic model.

We will now begin to use the `{tidymodel}` method for fitting models.


```{r logistic-model}
# The {tidymodels} method for logistic regression requires that the response be a factor variable
resume <- resume %>% 
  mutate(received_callback = as.factor(received_callback))

logistic_spec <- logistic_reg() %>%
  set_engine("glm")

logistic_spec

resume_mod <- logistic_spec %>%
  fit(received_callback ~ race, data = resume, family = "binomial")

tidy(resume_mod) %>% 
  knitr::kable(digits = 3)
```

The regression equation corresponding to résumés/persons perceived as White is received_callback = 0.438(racewhite) - 2.675.

The simplified estimated regression equation corresponding to résumés/persons perceived as Black is `received_callback = ???? (raceblack) + 2.675`

Based on the simplified regression equation for résumés/persons perceived as Black, the log-odds of being called back are the value of the intercept term, which is 
−2.675.

The odds that a randomly selected résumé/person perceived as Black will be called back are approximately 6.9%.

The probability that a randomly selected résumé/person perceived as Black will be called back is approximately 6.45%.


## Challenge: Extending to Mulitple Logistic Regression

We will explore the following question: Is there a difference in call back rates in Chicago jobs, after adjusting for the an applicant's years of experience, years of college, race, and gender?
Specifically, we will fit the following model, where $\hat{p}$ is the estimated probability of receiving a callback for a job in Chicago.

$$
\begin{equation*}
\log\left(\frac{\hat{p}}{1-\hat{p}}\right) = \hat\beta_0 + \hat\beta_1 \times (\texttt{years\\_experience}) + \hat\beta_2 \times (\texttt{race:White}) + \hat\beta_3 \times (\texttt{gender:male})
\end{equation*}
$$

Note that the researchers have the variable labeled `gender`.
Like with `race`, they limited their resume/name generation to only two categorizations: "male" and "female".
The authors do not address this decision in their article or provide any context as to what they mean by "gender".

```{r resume-subset}
resume_subet <- resume %>% 
  filter(job_city == "Chicago") %>% 
  mutate(race = case_when(
         race == "white" ~ "White",
         TRUE ~ "Black"
       ),
       gender = case_when(
         gender == "f" ~ "female",
         TRUE ~ "male"
       )) %>% 
  select(received_callback, years_experience, race, gender)
```

Here's a step-by-step explanation of what the code does:

Filtering: The filter(job_city == "Chicago") function call reduces the dataset to include only the rows where the job_city column's value is "Chicago". This step focuses the analysis on resumes submitted for jobs located in Chicago.

Mutating and Recoding race: The mutate() function is used to transform or add new columns to the dataset. Within this function, case_when() is used for conditional recoding of the race column:

If race equals "white", it is recoded to "White".

All other values of race (implicitly, those not equal to "white") are recoded to "Black".

This suggests the analysis is focused on comparing outcomes between two racial groups labeled "White" and "Black".

Mutating and Recoding gender: Still within the same mutate() call, gender is also recoded using case_when():

If gender equals "f", it is recoded to "female".

All other values of gender (implicitly, those not equal to "f") are recoded to "male".

This step simplifies gender into two categories: "male" and "female".

Selecting Specific Columns: Finally, the select(received_callback, years_experience, race, gender) function call trims the dataset down to include only the columns relevant for further analysis: whether the resume received a callback (received_callback), the years of experience (years_experience), and the recoded race and gender.

The resulting dataset, named resume_subet is a filtered and transformed version of the original resume dataset. It is now specifically tailored to analyze the effects of race and gender on the callback rates for job applications in Chicago, with a clear focus on comparing outcomes between these specified groups while controlling for years of experience. This streamlined dataset can then be used for statistical analysis or visualization to explore potential biases in callback rates based on the recoded race and gender categories.


## Relationship Exploration

There are many variables in this model. Let's explore each explanatory variable's relationship with the response variable.


```{r}
library(ggplot2)

ggplot(resume_subet, aes(x = received_callback, y = years_experience, fill = received_callback)) +
  geom_boxplot() +
  labs(title = "Years of Experience and Callback Status",
       x = "Callback Received",
       y = "Years of Experience") +
  theme_bw()
```

```{r}
# For Race
ggplot(resume_subet, aes(x = race, fill = received_callback)) +
  geom_bar(position = "fill") +
  labs(title = "Callback Rates by Race",
       x = "Race",
       y = "Proportion") +
  theme_bw()
```

```{r}
# For Gender
ggplot(resume_subet, aes(x = gender, fill = received_callback)) +
  geom_bar(position = "fill") +
  labs(title = "Callback Rates by Gender",
       x = "Gender",
       y = "Proportion") +
  theme_bw()

```

The three plots represent the relationship between years of experience, race, and gender with respect to callback status for job applications. Here are the observed patterns:

Years of Experience and Callback Status:

The box plot shows the distribution of years of experience for applicants who did not receive a callback (0) and those who did (1).

There's an overlap in the years of experience between those who received a callback and those who did not, suggesting that years of experience might not be a strong differentiator in the likelihood of receiving a callback.

The median years of experience for both groups are similar, although those who did receive a callback have a slightly higher interquartile range, suggesting a somewhat higher concentration of individuals with more experience in the callback group.

Callback Rates by Race:

The bar plot indicates the proportion of callbacks received by applicants perceived as Black and White.
There is a visible disparity in the callback rates by race. Applicants perceived as White have a higher proportion of callbacks compared to those perceived as Black, which suggests a potential racial bias in the callback rates.

Callback Rates by Gender:

The bar plot shows the callback rates broken down by gender.

The proportions of callbacks are similar across genders, with a slightly higher proportion for males compared to females. However, the difference is not as pronounced as the difference observed in callback rates by race.

Overall, race seems to play a more significant role than gender or years of experience in the likelihood of receiving a callback based on these visualizations. The data suggests potential racial bias in the hiring process, as indicated by the difference in callback rates between Black and White applicants. Meanwhile, gender does not seem to have as stark a difference in the callback rates, and the impact of years of experience is not clearly delineated from these plots alone. 

## Fitting the model

```{r}
mult_log_mod <- glm(received_callback ~ years_experience + race + gender, data = resume_subet, family = "binomial")

tidy(mult_log_mod) %>% 
  knitr::kable(digits = 3)
```

Focusing on the estimated coefficient for `years_experience`, we would say:

For each additional year of experience for an applicant in Chicago, we expect the *log odds* of an applicant receiving a call back to increase by 0.045 units.

Assuming applicants have similar time in spent in college, similar inferred races, and similar inferred gender.

This interpretation is somewhat confusing because we are describing this in *log odds*.
Fortunately, we can convert these back to odds using the following transformation:

$$
\text{odds} = e^{\log(\text{odds})}
$$
The estimated coefficient for years_experience in the logistic regression model is 0.045. This coefficient represents the change in the log-odds of the outcome variable (receiving a callback) for each one-unit increase in years of experience, holding all other variables constant.

# Assessing model fit

Now we want to check the residuals of this model to check the model's fit.

As we saw for multiple linear regression, there are various kinds of residuals that try to adjust for various features of the data. 
Two new residuals to explore are *Pearson residuals* and *Deviance residuals*.

**Pearson residuals**

The Pearson residual corrects for the unequal variance in the raw residuals by dividing by the standard deviation.

$$
\text{Pearson}_i = \frac{y_i - \hat{p}_i}{\sqrt{\hat{p}_i(1 - \hat{p}_i)}}
$$

**Deviance residuals**

Deviance residuals are popular because the sum of squares of these residuals is the deviance statistic.
We will talk more about this later in the semester.

$$
d_i = \text{sign}(y_i - \hat{p}_i)\sqrt{2\Big[y_i\log\Big(\frac{y_i}{\hat{p}_i}\Big) + (1 - y_i)\log\Big(\frac{1 - y_i}{1 - \hat{p}_i}\Big)\Big]}
$$

Since Pearson residuals are similar to residuals that we have already explored, we will instead focus on the deviance residuals.

```{r residual-plots}
# To store residuals and create row number variable
mult_log_aug <- augment(mult_log_mod, type.predict = "response", 
                      type.residuals = "deviance") %>% 
                      mutate(id = row_number())

# Plot residuals vs fitted values
ggplot(data = mult_log_aug, aes(x = .fitted, y = .resid)) + 
geom_point() + 
geom_hline(yintercept = 0, color = "red") + 
labs(x = "Fitted values", 
     y = "Deviance residuals", 
     title = "Deviance residuals vs. fitted")
     
# Plot residuals vs row number
ggplot(data = mult_log_aug, aes(x = id, y = .resid)) + 
geom_point() + 
geom_hline(yintercept = 0, color = "red") + 
labs(x = "id", 
     y = "Deviance residuals", 
     title = "Deviance residuals vs. id")
```

Here we produced two residual plots: the deviance residuals against the fitted values and the deviance variables against the index id (an index plot).

The index plot allows us to easily see some of the more extreme observations - there are a lot ($|d_i| > 2$ is quiet alarming).

The residual plot may look odd (why are there two distinct lines?!?), but this is a pretty typical shape when working with a binary response variable (the original data is really either a 0 or a 1).

In general because there are so many extreme values in the index plot, this model leaves room for improvement.
